{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install llama-index llama-index-llms-huggingface llama-index-embeddings-huggingface llama-index-readers-web transformers accelerate -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import ServiceContext\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.llms.huggingface import HuggingFaceLLM\n",
    "from llama_index.core.prompts.base import PromptTemplate\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.readers.web import BeautifulSoupWebReader\n",
    "from os import environ\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__CUDNN VERSION: 3001000\n",
      "__Number CUDA Devices: 1\n"
     ]
    }
   ],
   "source": [
    "environ[\"HIP_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "if use_cuda:\n",
    "    print('__CUDNN VERSION:', torch.backends.cudnn.version())\n",
    "    print('__Number CUDA Devices:', torch.cuda.device_count())\n",
    "    count = torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def messages_to_prompt(messages):\n",
    "  prompt = \"\"\n",
    "  for message in messages:\n",
    "    if message.role == 'system':\n",
    "      prompt += f\"<|system|>\\n{message.content}</s>\\n\"\n",
    "    elif message.role == 'user':\n",
    "      prompt += f\"<|user|>\\n{message.content}</s>\\n\"\n",
    "    elif message.role == 'assistant':\n",
    "      prompt += f\"<|assistant|>\\n{message.content}</s>\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading shards: 100%|██████████| 8/8 [06:28<00:00, 48.56s/it]\n",
      "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Loading checkpoint shards: 100%|██████████| 8/8 [00:11<00:00,  1.48s/it]\n"
     ]
    }
   ],
   "source": [
    "llm = HuggingFaceLLM(\n",
    "    model_name=\"HuggingFaceH4/zephyr-7b-alpha\",\n",
    "    tokenizer_name=\"HuggingFaceH4/zephyr-7b-alpha\",\n",
    "    query_wrapper_prompt=PromptTemplate(\"<|system|>\\n</s>\\n<|user|>\\n{query_str}</s>\\n<|assistant|>\\n\"),\n",
    "    context_window=3900,\n",
    "    max_new_tokens=256,\n",
    "    model_kwargs={\"use_safetensors\": False},\n",
    "    # tokenizer_kwargs={},\n",
    "    generate_kwargs={\"do_sample\":True, \"temperature\": 0.7, \"top_k\": 50, \"top_p\": 0.95},\n",
    "    messages_to_prompt=messages_to_prompt,\n",
    "    device_map=\"cuda\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paul Graham's recommendations for working hard are as follows:\n",
      "\n",
      "1. Work on what interests you: Choose a project or task that excites you and makes you want to put in extra effort.\n",
      "\n",
      "2. Make it a habit: Set aside a specific time each day or week for working on your project. This will help make it a habit and a part of your daily routine.\n",
      "\n",
      "3. Set goals: Set specific and measurable goals for what you want to achieve. This will help you stay focused and motivated.\n",
      "\n",
      "4. Eliminate distractions: Remove any distractions that might prevent you from working effectively. This could mean turning off your phone, closing unnecessary tabs on your computer, or finding a quiet space to work.\n",
      "\n",
      "5. Work smarter, not harder: Focus on working efficiently and effectively, rather than simply working longer hours. This will help you get more done in less time.\n",
      "\n",
      "6. Take breaks: Take regular breaks to recharge your batteries and avoid burnout. This will help you stay focused and productive over the long term.\n",
      "\n",
      "7. Learn to say no: Don't overcommit yourself to too many projects or tasks. Learn to say no to projects or tasks\n"
     ]
    }
   ],
   "source": [
    "question = \"How does Paul Graham recommend to work hard? Can you list it as steps\"\n",
    "response = llm.complete(question)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://paulgraham.com/hwh.html\"\n",
    "\n",
    "documents = BeautifulSoupWebReader().load_data([url])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = documents[0].text\n",
    "prompt = f\"\"\"Answer the question based on the context below. If the\n",
    "question cannot be answered using the information provided answer\n",
    "with \"I don't know\".\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer: \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paul Graham suggests the following steps to work hard:\n",
      "\n",
      "1. Learn what real work is by distinguishing it from the distorted versions of it you may have encountered in school or in certain types of work.\n",
      "\n",
      "2. Find the limit of working hard by noticing if the quality of the work decreases beyond a certain number of hours per day.\n",
      "\n",
      "3. Constantly judge both how hard you're trying and how well you're doing, and aim toward the center of the problem, rather than merely pushing yourself to work.\n",
      "\n",
      "4. Figure out which type of work you're suited for by understanding the shape of real work and which kind interests you, rather than just following your talents.\n",
      "\n",
      "5. Figure out what to work on by being honest with yourself about your interests, rather than just following the current consensus about which problems are most important.\n",
      "\n",
      "6. Give yourself time to get going when working on a new type of work, but also be prepared to switch fields if you're not getting good enough results.\n",
      "\n",
      "7. Find what you find most interesting when trying to decide whether to work on something, rather than just focusing on how much money you can make or how impressive others will find it.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = llm.complete(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py_3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
